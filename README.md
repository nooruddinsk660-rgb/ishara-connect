
# Ishara Connect - AI Sign Language Interpreter ğŸ–ï¸ğŸ¤–

**Ishara Connect** is an advanced, real-time sign language interpretation system powered by AI and Computer Vision. It translates hand gestures into spoken language (Bengali, Hindi, English) to bridge the communication gap for the deaf and mute community.


## ğŸŒŸ Key Features

*   **Real-Time Gesture Recognition**: Uses MediaPipe and a custom Random Forest model to detect 30+ gestures instantly.
*   **Multi-Language Support**: Translates gestures into **Bengali**, **Hindi**, and **English**.
*   **Polite Mode**: Toggle between casual and formal/polite speech output (e.g., "Water" vs. "Could I please have some water?").
*   **Cyberpunk Glassmorphism UI**: A stunning, modern interface with neon accents, animated gradients, and glassmorphism cards.
*   **Robotic Hand Visualization**: Features a high-tech, glowing robotic hand tracking effect in the video feed.
*   **Voice & Camera Controls**: Integrated toggle buttons to mute audio or turn off the camera feed.
*   **Theme System**: Switch between a dark "Cyberpunk" theme and a clean "Light" mode.

## ğŸ› ï¸ Tech Stack

*   **Frontend**: HTML5, CSS3 (Glassmorphism, Animations), JavaScript (Fetch API)
*   **Backend**: Python (Flask)
*   **AI/ML**: OpenCV, MediaPipe, Scikit-Learn (Random Forest), NumPy, Pandas
*   **Audio**: gTTS (Google Text-to-Speech) / Pre-generated MP3s

## ğŸ“‚ Project Structure

```
ishara-connect/
â”œâ”€â”€ app.py                  # Main Flask application & Inference logic
â”œâ”€â”€ data_collector.py       # Script to collect training data via webcam
â”œâ”€â”€ train_model.py          # Script to train the Random Forest model
â”œâ”€â”€ utils.py                # Helper functions (feature extraction)
â”œâ”€â”€ generate_premium_audio.py # Script to generate TTS audio files
â”œâ”€â”€ model.p                 # Trained AI Model (Pickle file)
â”œâ”€â”€ data.csv                # Collected dataset (Features & Labels)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/                # Stylesheets (style.css)
â”‚   â”œâ”€â”€ audio/              # Generated audio files (organized by language)
â”‚   â””â”€â”€ images/             # Static assets
â””â”€â”€ templates/
    â”œâ”€â”€ index.html          # Main dashboard
    â””â”€â”€ components/         # HTML components (header, camera, gestures, scripts)
```

## ğŸš€ Getting Started

### Prerequisites

*   Python 3.8+
*   Webcam

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/yourusername/ishara-connect.git
    cd ishara-connect
    ```

2.  **Install Dependencies**:
    ```bash
    pip install flask opencv-python mediapipe scikit-learn pandas numpy gtts
    ```

### Usage

1.  **Run the Application**:
    ```bash
    python app.py
    ```

2.  **Open in Browser**:
    Navigate to `http://127.0.0.1:5000`

3.  **Interact**:
    *   Show gestures to the camera.
    *   Click the **Language Icon** to change languages.
    *   Click the **Polite Toggle** for formal speech.
    *   Use the **Camera/Sound** buttons to control the feed/audio.

## ğŸ§  Training Your Own Gestures

1.  **Configure Classes**:
    Open `data_collector.py` and modify `TARGET_CLASSES` to include the gestures you want to record.
    ```python
    TARGET_CLASSES = ["MyNewGesture"]
    ```

2.  **Collect Data**:
    Run the collector and follow the on-screen prompts.
    ```bash
    python data_collector.py
    ```

3.  **Train Model**:
    Run the training script to update `model.p`.
    ```bash
    python train_model.py
    ```

## ğŸ¤ Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## ğŸ“„ License

This project is licensed under the MIT License.
